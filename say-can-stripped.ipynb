{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfYKd3ZPB6s_"
      },
      "source": [
        "### Copyright 2022 Google LLC. SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLzMRizDB2l5"
      },
      "source": [
        "Copyright 2022 Google LLC. SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GrV-nN-CTpl"
      },
      "source": [
        "# SayCan on a Robot Pick and Place Tabletop Environment\n",
        "\n",
        "[SayCan](https://say-can.github.io/) is an algorithm that grounds large language models with robotic affordances for long-horizon planning. Given a set of low-level robotic skills (e.g., \"put the green block in the red bowl\") and a high-level instruction (e.g., \"stack all the blocks\"), it scores what a language model believes will help forward the high-level instruction and scores what a robotic affordance model believes is possible. Together these give a task that is useful and possible and the robot executes the command.\n",
        "\n",
        "<img src=\"https://github.com/say-can/say-can.github.io/blob/main/img/saycan.png?raw=true\" height=\"320px\">\n",
        "\n",
        "This colab runs an example of SayCan for a pick and place robot on a table top.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/say-can/say-can.github.io/main/img/open_source_tabletop.png\" height=\"320px\">\n",
        "\n",
        "Models used: [GPT-3](https://arxiv.org/abs/2005.14165) (InstructGPT), [CLIP](https://arxiv.org/abs/2103.00020) (ViT-B/32), [ViLD](https://arxiv.org/abs/2104.13921), and [CLIPort](https://cliport.github.io/) variant ([Transporter Nets](https://transporternets.github.io/))\n",
        "\n",
        "### **Quick Start on Google Collab:**\n",
        "\n",
        "**Step 1.** Register for an [OpenAI API key](https://openai.com/blog/openai-api/) to use GPT-3 (there's a free trial) and enter it below\n",
        "\n",
        "**Step 2.** Menu > Change runtime type > Hardware accelerator > \"GPU\"\n",
        "\n",
        "**Step 3.** Menu > Runtime > Run all\n",
        "\n",
        "### **Quick Start on VScode (most Mya tested)**\n",
        "**Prereqs** If you want to use a nvidia GPU, you need to have CUDA 12.X to work with this or you need to change the version of tensorflow.\n",
        "\n",
        "**Step 1.** I would recommend starting your kernel in a python venv in the current directory. \n",
        "\n",
        "**Step 2.** Menu bar > Run all\n",
        "\n",
        "\n",
        "### Troubleshooting on Ubuntu VScode\n",
        "* `OSError: Unable to download 'ffmpeg-linux64-v3.3.1'. Perhaps there is a no internet connection? If there is, please report this problem.`\n",
        "    * To fix this, run the following. NOTE: If you have an architecture different than X86 64bit replace `ffmpeg-linux-x86_64-v7.0.2` in both commands below with the name version that matches your architecture [from here](https://github.com/imageio/imageio-binaries/tree/master/ffmpeg).\n",
        "\n",
        "        ```bash\n",
        "        wget https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux-x86_64-v7.0.2\n",
        "        mv ffmpeg-linux-x86_64-v7.0.2 ~/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1\n",
        "        ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OI-l5UUxZf_I"
      },
      "outputs": [],
      "source": [
        "ENGINE = \"gpt-4o-mini\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2UdCCcpwbl0"
      },
      "source": [
        "## **Setup**\n",
        "This does a few things:\n",
        "- Installs dependencies: CLIP, PyTorch, etc.\n",
        "- Downloads demo videos, models, and cached LLM calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F2mFnHhPSTd_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in ./.venv/lib/python3.10/site-packages (6.3.1)\n",
            "Requirement already satisfied: regex in ./.venv/lib/python3.10/site-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.67.1)\n",
            "Requirement already satisfied: fvcore in ./.venv/lib/python3.10/site-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: imageio==2.4.1 in ./.venv/lib/python3.10/site-packages (2.4.1)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.4.5 in ./.venv/lib/python3.10/site-packages (0.4.5)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from imageio==2.4.1) (1.26.4)\n",
            "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (from imageio==2.4.1) (11.1.0)\n",
            "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: yacs>=0.1.6 in ./.venv/lib/python3.10/site-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1 in ./.venv/lib/python3.10/site-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: tabulate in ./.venv/lib/python3.10/site-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in ./.venv/lib/python3.10/site-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing_extensions in ./.venv/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Requirement already satisfied: portalocker in ./.venv/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (3.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-k5a0jomy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-k5a0jomy\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: ftfy in ./.venv/lib/python3.10/site-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in ./.venv/lib/python3.10/site-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from clip==1.0) (2.5.1)\n",
            "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (from clip==1.0) (0.20.1)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: gdown in ./.venv/lib/python3.10/site-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.10/site-packages (from gdown) (4.13.0b3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in ./.venv/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pybullet in ./.venv/lib/python3.10/site-packages (3.2.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: moviepy==0.2.3.5 in ./.venv/lib/python3.10/site-packages (0.2.3.5)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in ./.venv/lib/python3.10/site-packages (from moviepy==0.2.3.5) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in ./.venv/lib/python3.10/site-packages (from moviepy==0.2.3.5) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in ./.venv/lib/python3.10/site-packages (from moviepy==0.2.3.5) (4.67.1)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from moviepy==0.2.3.5) (1.26.4)\n",
            "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (from imageio<3.0,>=2.1.2->moviepy==0.2.3.5) (11.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: chex==0.1.81 in ./.venv/lib/python3.10/site-packages (0.1.81)\n",
            "Requirement already satisfied: optax==0.1.5 in ./.venv/lib/python3.10/site-packages (0.1.5)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in ./.venv/lib/python3.10/site-packages (from chex==0.1.81) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.10/site-packages (from chex==0.1.81) (4.12.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in ./.venv/lib/python3.10/site-packages (from chex==0.1.81) (0.1.8)\n",
            "Requirement already satisfied: jax>=0.4.6 in ./.venv/lib/python3.10/site-packages (from chex==0.1.81) (0.4.6)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in ./.venv/lib/python3.10/site-packages (from chex==0.1.81) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.25.0 in ./.venv/lib/python3.10/site-packages (from chex==0.1.81) (1.26.4)\n",
            "Requirement already satisfied: toolz>=0.9.0 in ./.venv/lib/python3.10/site-packages (from chex==0.1.81) (1.0.0)\n",
            "Requirement already satisfied: opt_einsum in ./.venv/lib/python3.10/site-packages (from jax>=0.4.6->chex==0.1.81) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.5 in ./.venv/lib/python3.10/site-packages (from jax>=0.4.6->chex==0.1.81) (1.12.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: jax==0.4.6 in ./.venv/lib/python3.10/site-packages (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.20 in ./.venv/lib/python3.10/site-packages (from jax==0.4.6) (1.26.4)\n",
            "Requirement already satisfied: opt_einsum in ./.venv/lib/python3.10/site-packages (from jax==0.4.6) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.5 in ./.venv/lib/python3.10/site-packages (from jax==0.4.6) (1.12.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: jaxlib==0.4.6 in ./.venv/lib/python3.10/site-packages (0.4.6)\n",
            "Requirement already satisfied: scipy>=1.5 in ./.venv/lib/python3.10/site-packages (from jaxlib==0.4.6) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.20 in ./.venv/lib/python3.10/site-packages (from jaxlib==0.4.6) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: flax==0.5.3 in ./.venv/lib/python3.10/site-packages (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.12 in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (1.26.4)\n",
            "Requirement already satisfied: jax>=0.3.2 in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (0.4.6)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (3.10.0)\n",
            "Requirement already satisfied: msgpack in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (1.1.0)\n",
            "Requirement already satisfied: optax in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (0.1.5)\n",
            "Requirement already satisfied: rich~=11.1 in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (11.2.0)\n",
            "Requirement already satisfied: tensorstore in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (0.1.71)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.10/site-packages (from flax==0.5.3) (6.0.2)\n",
            "Requirement already satisfied: opt_einsum in ./.venv/lib/python3.10/site-packages (from jax>=0.3.2->flax==0.5.3) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.5 in ./.venv/lib/python3.10/site-packages (from jax>=0.3.2->flax==0.5.3) (1.12.0)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from rich~=11.1->flax==0.5.3) (0.4.6)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./.venv/lib/python3.10/site-packages (from rich~=11.1->flax==0.5.3) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./.venv/lib/python3.10/site-packages (from rich~=11.1->flax==0.5.3) (2.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->flax==0.5.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->flax==0.5.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->flax==0.5.3) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->flax==0.5.3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->flax==0.5.3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib->flax==0.5.3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->flax==0.5.3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib->flax==0.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in ./.venv/lib/python3.10/site-packages (from optax->flax==0.5.3) (2.1.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in ./.venv/lib/python3.10/site-packages (from optax->flax==0.5.3) (0.1.81)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in ./.venv/lib/python3.10/site-packages (from optax->flax==0.5.3) (0.4.6)\n",
            "Requirement already satisfied: ml_dtypes>=0.3.1 in ./.venv/lib/python3.10/site-packages (from tensorstore->flax==0.5.3) (0.4.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in ./.venv/lib/python3.10/site-packages (from chex>=0.1.5->optax->flax==0.5.3) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in ./.venv/lib/python3.10/site-packages (from chex>=0.1.5->optax->flax==0.5.3) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->flax==0.5.3) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: openai in ./.venv/lib/python3.10/site-packages (1.60.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.10/site-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.10/site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: easydict in ./.venv/lib/python3.10/site-packages (1.13)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: imageio-ffmpeg in ./.venv/lib/python3.10/site-packages (0.4.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.10/site-packages (1.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scipy==1.12.0 in ./.venv/lib/python3.10/site-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in ./.venv/lib/python3.10/site-packages (from scipy==1.12.0) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: protobuf==3.20.3 in ./.venv/lib/python3.10/site-packages (3.20.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow==2.18.0 in ./.venv/lib/python3.10/site-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (59.6.0)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.18.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n",
            "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (11.2.0)\n",
            "Requirement already satisfied: namex in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.8)\n",
            "Requirement already satisfied: optree in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (0.4.6)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.19.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: numpy==1.26.4 in ./.venv/lib/python3.10/site-packages (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mya/Desktop/github/HCI/mya-plan/.venv/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/home/mya/Desktop/github/HCI/mya-plan/.venv/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "2025-01-29 08:42:12.944214: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738161732.974911    7385 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738161732.984848    7385 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: gsutil: command not found\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pybullet build time: Nov 28 2023 23:45:17\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jan 29 08:42:15 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0  On |                  N/A |\n",
            "| N/A   53C    P4             13W /   50W |     502MiB /   8188MiB |     18%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      3166      G   /usr/lib/xorg/Xorg                            183MiB |\n",
            "|    0   N/A  N/A      3946      G   /usr/bin/gnome-shell                           89MiB |\n",
            "|    0   N/A  N/A      4766      G   ...seed-version=20250128-180236.310000        104MiB |\n",
            "|    0   N/A  N/A      6344      G   ...erProcess --variations-seed-version         86MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "%pip install ftfy regex tqdm fvcore imageio==2.4.1 imageio-ffmpeg==0.4.5\n",
        "%pip install git+https://github.com/openai/CLIP.git\n",
        "%pip install -U --no-cache-dir gdown --pre\n",
        "%pip install pybullet \n",
        "%pip install moviepy==0.2.3.5 # Need to downgrade to work with imageio 2.4.1\n",
        "%pip install chex==0.1.81 optax==0.1.5 # Need to downgrade to work with jax\n",
        "%pip install jax==0.4.6 # Need to downgrade to work with code\n",
        "%pip install jaxlib==0.4.6 # Need to downgrade to work with jax\n",
        "%pip install flax==0.5.3\n",
        "%pip install openai\n",
        "%pip install easydict\n",
        "%pip install imageio-ffmpeg\n",
        "%pip install opencv-python\n",
        "%pip install python-dotenv\n",
        "%pip install scipy==1.12.0\n",
        "%pip install protobuf==3.20.3 #3.20.0\n",
        "%pip install typing-extensions>=4.12.2\n",
        "%pip install tensorflow==2.18.0 #2.8.0  # Updated from 2.7.0 so that it is compatible with  CUDA 12.X\n",
        "%pip install numpy==1.26.4\n",
        "\n",
        "\n",
        "\n",
        "import collections\n",
        "import datetime\n",
        "import os\n",
        "import random\n",
        "import threading\n",
        "import time\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import cv2  # Used by ViLD.\n",
        "# import clip\n",
        "# from easydict import EasyDict\n",
        "# import flax\n",
        "# from flax import linen as nn\n",
        "# from flax.training import checkpoints\n",
        "# from flax.metrics import tensorboard\n",
        "import imageio\n",
        "from heapq import nlargest\n",
        "# import IPython\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "# from moviepy.editor import ImageSequenceClip\n",
        "import numpy as np\n",
        "import openai\n",
        "# import optax\n",
        "# import pickle\n",
        "# from PIL import Image\n",
        "# import pybullet\n",
        "# import pybullet_data\n",
        "import tensorflow.compat.v1 as tf\n",
        "# import torch\n",
        "# from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Load API key\n",
        "load_dotenv()\n",
        "openai_api_key = os.getenv('OPENAI_API')\n",
        "\n",
        "\n",
        "#Download PyBullet assets.\n",
        "if not os.path.exists('ur5e/ur5e.urdf'):\n",
        "  !gdown --id 1Cc_fDSBL6QiDvNT4dpfAEbhbALSVoWcc\n",
        "  !gdown --id 1yOMEm-Zp_DL3nItG9RozPeJAmeOldekX\n",
        "  !gdown --id 1GsqNLhEl9dd4Mc3BM0dX3MibOI1FVWNM\n",
        "  !unzip ur5e.zip\n",
        "  !unzip robotiq_2f_85.zip\n",
        "  !unzip bowl.zip\n",
        "\n",
        "# ViLD pretrained model weights.\n",
        "!gsutil cp -r gs://cloud-tpu-checkpoints/detection/projects/vild/colab/image_path_v2 ./\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Show useful GPU info.\n",
        "!nvidia-smi\n",
        "\n",
        "# Show if JAX is using GPU.\n",
        "from jax.lib import xla_bridge\n",
        "print(xla_bridge.get_backend().platform)\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment\n",
        "Define PyBullet-based environment with a UR5e and 2-finger gripper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@markdown Global constants: pick and place objects, colors, workspace bounds\n",
        "\n",
        "PICK_TARGETS = {\n",
        "  \"blue block\": None,\n",
        "  \"red block\": None,\n",
        "  \"green block\": None,\n",
        "  \"yellow block\": None,\n",
        "}\n",
        "\n",
        "COLORS = {\n",
        "    \"blue\":   (78/255,  121/255, 167/255, 255/255),\n",
        "    \"red\":    (255/255,  87/255,  89/255, 255/255),\n",
        "    \"green\":  (89/255,  169/255,  79/255, 255/255),\n",
        "    \"yellow\": (237/255, 201/255,  72/255, 255/255),\n",
        "}\n",
        "\n",
        "PLACE_TARGETS = {\n",
        "  \"blue block\": None,\n",
        "  \"red block\": None,\n",
        "  \"green block\": None,\n",
        "  \"yellow block\": None,\n",
        "\n",
        "  \"blue bowl\": None,\n",
        "  \"red bowl\": None,\n",
        "  \"green bowl\": None,\n",
        "  \"yellow bowl\": None,\n",
        "\n",
        "  \"top left corner\":     (-0.3 + 0.05, -0.2 - 0.05, 0),\n",
        "  \"top right corner\":    (0.3 - 0.05,  -0.2 - 0.05, 0),\n",
        "  \"middle\":              (0,           -0.5,        0),\n",
        "  \"bottom left corner\":  (-0.3 + 0.05, -0.8 + 0.05, 0),\n",
        "  \"bottom right corner\": (0.3 - 0.05,  -0.8 + 0.05, 0),\n",
        "}\n",
        "\n",
        "PIXEL_SIZE = 0.00267857\n",
        "BOUNDS = np.float32([[-0.3, 0.3], [-0.8, -0.2], [0, 0.15]])  # X Y Z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Iyw8Zs3Sn5"
      },
      "source": [
        "## Setup SayCan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hPk9A3lQgtcy"
      },
      "outputs": [],
      "source": [
        "#@title LLM Cache\n",
        "overwrite_cache = True\n",
        "if overwrite_cache:\n",
        "  LLM_CACHE = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u1_l-NtYRHVC"
      },
      "outputs": [],
      "source": [
        "# with open('filename.pickle', 'wb') as handle:\n",
        "#     pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# # with open('llm_cache.pickle', 'rb') as handle:\n",
        "# #     b = pickle.load(LLM_CACHE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cn53B4YsII3a"
      },
      "outputs": [],
      "source": [
        "#@title LLM Scoring\n",
        "\n",
        "def gpt3_call(engine=\"text-ada-001\", prompt=\"\", max_tokens=128, temperature=0, \n",
        "              logprobs=1, echo=False):\n",
        "  full_query = \"\"\n",
        "  for p in prompt:\n",
        "    full_query += p\n",
        "  id = tuple((engine, full_query, max_tokens, temperature, logprobs, echo))\n",
        "  if id in LLM_CACHE.keys():\n",
        "    print('cache hit, returning')\n",
        "    response = LLM_CACHE[id]\n",
        "  else:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=engine,  # Example: \"gpt-4\" or \"gpt-3.5-turbo\"\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    print(response)\n",
        "\n",
        "    LLM_CACHE[id] = response\n",
        "  return response\n",
        "\n",
        "def gpt3_scoring(query, options, engine=\"text-ada-001\", limit_num_options=None, option_start=\"\\n\", verbose=False, print_tokens=False):\n",
        "  if limit_num_options:\n",
        "    options = options[:limit_num_options]\n",
        "  verbose and print(\"Scoring\", len(options), \"options\")\n",
        "  gpt3_prompt_options = [query + option for option in options]\n",
        "  response = gpt3_call(\n",
        "      engine=engine, \n",
        "      prompt=gpt3_prompt_options, \n",
        "      max_tokens=0,\n",
        "      logprobs=1, \n",
        "      temperature=0,\n",
        "      echo=True,)\n",
        "  \n",
        "  scores = {}\n",
        "  for option, choice in zip(options, response[\"choices\"]):\n",
        "    tokens = choice[\"logprobs\"][\"tokens\"]\n",
        "    token_logprobs = choice[\"logprobs\"][\"token_logprobs\"]\n",
        "\n",
        "    total_logprob = 0\n",
        "    for token, token_logprob in zip(reversed(tokens), reversed(token_logprobs)):\n",
        "      print_tokens and print(token, token_logprob)\n",
        "      if option_start is None and not token in option:\n",
        "        break\n",
        "      if token == option_start:\n",
        "        break\n",
        "      total_logprob += token_logprob\n",
        "    scores[option] = total_logprob\n",
        "\n",
        "  for i, option in enumerate(sorted(scores.items(), key=lambda x : -x[1])):\n",
        "    verbose and print(option[1], \"\\t\", option[0])\n",
        "    if i >= 10:\n",
        "      break\n",
        "\n",
        "  return scores, response\n",
        "\n",
        "def make_options(pick_targets=None, place_targets=None, options_in_api_form=True, termination_string=\"done()\"):\n",
        "  if not pick_targets:\n",
        "    pick_targets = PICK_TARGETS\n",
        "  if not place_targets:\n",
        "    place_targets = PLACE_TARGETS\n",
        "  options = []\n",
        "  for pick in pick_targets:\n",
        "    for place in place_targets:\n",
        "      if options_in_api_form:\n",
        "        option = \"robot.pick_and_place({}, {})\".format(pick, place)\n",
        "      else:\n",
        "        option = \"Pick the {} and place it on the {}.\".format(pick, place)\n",
        "      options.append(option)\n",
        "\n",
        "  options.append(termination_string)\n",
        "  print(\"Considering\", len(options), \"options\")\n",
        "  return options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Shp3CFFDzDie"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: openai\n",
            "Version: 1.60.2\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License-Expression: Apache-2.0\n",
            "Location: /home/mya/Desktop/github/HCI/mya-plan/.venv/lib/python3.10/site-packages\n",
            "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: \n",
            "Considering 53 options\n",
            "Scoring 5 options\n"
          ]
        },
        {
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo pick the blue block and put it on the red block, I should:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m options \u001b[38;5;241m=\u001b[39m make_options(PICK_TARGETS, PLACE_TARGETS)\n\u001b[0;32m----> 4\u001b[0m scores, response \u001b[38;5;241m=\u001b[39m \u001b[43mgpt3_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENGINE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_num_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36mgpt3_scoring\u001b[0;34m(query, options, engine, limit_num_options, option_start, verbose, print_tokens)\u001b[0m\n\u001b[1;32m     31\u001b[0m verbose \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScoring\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(options), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m gpt3_prompt_options \u001b[38;5;241m=\u001b[39m [query \u001b[38;5;241m+\u001b[39m option \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m options]\n\u001b[0;32m---> 33\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgpt3_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt3_prompt_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m option, choice \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(options, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
            "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mgpt3_call\u001b[0;34m(engine, prompt, max_tokens, temperature, logprobs, echo)\u001b[0m\n\u001b[1;32m     11\u001b[0m   response \u001b[38;5;241m=\u001b[39m LLM_CACHE[\u001b[38;5;28mid\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Example: \"gpt-4\" or \"gpt-3.5-turbo\"\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m          \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m   \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     25\u001b[0m   LLM_CACHE[\u001b[38;5;28mid\u001b[39m] \u001b[38;5;241m=\u001b[39m response\n",
            "File \u001b[0;32m~/Desktop/github/HCI/mya-plan/.venv/lib/python3.10/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip show openai\n",
        "query = \"To pick the blue block and put it on the red block, I should:\\n\"\n",
        "options = make_options(PICK_TARGETS, PLACE_TARGETS)\n",
        "scores, response = gpt3_scoring(query, options, engine=ENGINE, limit_num_options=5, option_start='\\n', verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "w16kuljCGd2o"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions\n",
        "\n",
        "def build_scene_description(found_objects, block_name=\"box\", bowl_name=\"circle\"):\n",
        "  scene_description = f\"objects = {found_objects}\"\n",
        "  scene_description = scene_description.replace(block_name, \"block\")\n",
        "  scene_description = scene_description.replace(bowl_name, \"bowl\")\n",
        "  scene_description = scene_description.replace(\"'\", \"\")\n",
        "  return scene_description\n",
        "\n",
        "def step_to_nlp(step):\n",
        "  step = step.replace(\"robot.pick_and_place(\", \"\")\n",
        "  step = step.replace(\")\", \"\")\n",
        "  pick, place = step.split(\", \")\n",
        "  return \"Pick the \" + pick + \" and place it on the \" + place + \".\"\n",
        "\n",
        "def normalize_scores(scores):\n",
        "  max_score = max(scores.values())  \n",
        "  normed_scores = {key: np.clip(scores[key] / max_score, 0, 1) for key in scores}\n",
        "  return normed_scores\n",
        "\n",
        "def plot_saycan(llm_scores, vfs, combined_scores, task, correct=True, show_top=None):\n",
        "  if show_top:\n",
        "    top_options = nlargest(show_top, combined_scores, key = combined_scores.get)\n",
        "    # add a few top llm options in if not already shown\n",
        "    top_llm_options = nlargest(show_top // 2, llm_scores, key = llm_scores.get)\n",
        "    for llm_option in top_llm_options:\n",
        "      if not llm_option in top_options:\n",
        "        top_options.append(llm_option)\n",
        "    llm_scores = {option: llm_scores[option] for option in top_options}\n",
        "    vfs = {option: vfs[option] for option in top_options}\n",
        "    combined_scores = {option: combined_scores[option] for option in top_options}\n",
        "\n",
        "  sorted_keys = dict(sorted(combined_scores.items()))\n",
        "  keys = [key for key in sorted_keys]\n",
        "  positions = np.arange(len(combined_scores.items()))\n",
        "  width = 0.3\n",
        "\n",
        "  fig = plt.figure(figsize=(12, 6))\n",
        "  ax1 = fig.add_subplot(1,1,1)\n",
        "\n",
        "  plot_llm_scores = normalize_scores({key: np.exp(llm_scores[key]) for key in sorted_keys})\n",
        "  plot_llm_scores = np.asarray([plot_llm_scores[key] for key in sorted_keys])\n",
        "  plot_affordance_scores = np.asarray([vfs[key] for key in sorted_keys])\n",
        "  plot_combined_scores = np.asarray([combined_scores[key] for key in sorted_keys])\n",
        "  \n",
        "  ax1.bar(positions, plot_combined_scores, 3 * width, alpha=0.6, color=\"#93CE8E\", label=\"combined\")\n",
        "    \n",
        "  score_colors = [\"#ea9999ff\" for score in plot_affordance_scores]\n",
        "  ax1.bar(positions + width / 2, 0 * plot_combined_scores, width, color=\"#ea9999ff\", label=\"vfs\")\n",
        "  ax1.bar(positions + width / 2, 0 * plot_combined_scores, width, color=\"#a4c2f4ff\", label=\"language\")\n",
        "  ax1.bar(positions - width / 2, np.abs(plot_affordance_scores), width, color=score_colors)\n",
        "  \n",
        "  plt.xticks(rotation=\"vertical\")\n",
        "  ax1.set_ylim(0.0, 1.0)\n",
        "\n",
        "  ax1.grid(True, which=\"both\")\n",
        "  ax1.axis(\"on\")\n",
        "\n",
        "  ax1_llm = ax1.twinx()\n",
        "  ax1_llm.bar(positions + width / 2, plot_llm_scores, width, color=\"#a4c2f4ff\", label=\"language\")\n",
        "  ax1_llm.set_ylim(0.01, 1.0)\n",
        "  plt.yscale(\"log\")\n",
        "  \n",
        "  font = {\"fontname\":\"Arial\", \"size\":\"16\", \"color\":\"k\" if correct else \"r\"}\n",
        "  plt.title(task, **font)\n",
        "  key_strings = [key.replace(\"robot.pick_and_place\", \"\").replace(\", \", \" to \").replace(\"(\", \"\").replace(\")\",\"\") for key in keys]\n",
        "  plt.xticks(positions, key_strings, **font)\n",
        "  ax1.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyQ8GVvzyxP5"
      },
      "outputs": [],
      "source": [
        "#@title Affordance Scoring\n",
        "#@markdown Given this environment does not have RL-trained policies or an asscociated value function, we use affordances through an object detector.\n",
        "\n",
        "def affordance_scoring(options, found_objects, verbose=False, block_name=\"box\", bowl_name=\"circle\", termination_string=\"done()\"):\n",
        "  affordance_scores = {}\n",
        "  found_objects = [\n",
        "                   found_object.replace(block_name, \"block\").replace(bowl_name, \"bowl\") \n",
        "                   for found_object in found_objects + list(PLACE_TARGETS.keys())[-5:]]\n",
        "  verbose and print(\"found_objects\", found_objects)\n",
        "  for option in options:\n",
        "    if option == termination_string:\n",
        "      affordance_scores[option] = 0.2\n",
        "      continue\n",
        "    pick, place = option.replace(\"robot.pick_and_place(\", \"\").replace(\")\", \"\").split(\", \")\n",
        "    affordance = 0\n",
        "    found_objects_copy = found_objects.copy()\n",
        "    if pick in found_objects_copy:\n",
        "      found_objects_copy.remove(pick)\n",
        "      if place in found_objects_copy:\n",
        "        affordance = 1\n",
        "    affordance_scores[option] = affordance\n",
        "    verbose and print(affordance, '\\t', option)\n",
        "  return affordance_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xZm83muyRWR"
      },
      "outputs": [],
      "source": [
        "#@title Test\n",
        "termination_string = \"done()\"\n",
        "query = \"To pick the blue block and put it on the red block, I should:\\n\"\n",
        "\n",
        "options = make_options(PICK_TARGETS, PLACE_TARGETS, termination_string=termination_string)\n",
        "llm_scores, _ = gpt3_scoring(query, options, verbose=True, engine=ENGINE)\n",
        "\n",
        "affordance_scores = affordance_scoring(options, found_objects, block_name=\"box\", bowl_name=\"circle\", verbose=False, termination_string=termination_string)\n",
        "\n",
        "combined_scores = {option: np.exp(llm_scores[option]) * affordance_scores[option] for option in options}\n",
        "combined_scores = normalize_scores(combined_scores)\n",
        "selected_task = max(combined_scores, key=combined_scores.get)\n",
        "print(\"Selecting: \", selected_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUQ71ADJ-HnK"
      },
      "source": [
        "# Demos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJVQIUvI77bF"
      },
      "source": [
        "## **SayCan**\n",
        "\n",
        "Here we implement SayCan with LLM scoring and robotic affordances from ViLD (in the absence of a trained value function from an RL policy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yys24Oj1796Y"
      },
      "outputs": [],
      "source": [
        "#@title Prompt\n",
        "\n",
        "termination_string = \"done()\"\n",
        "\n",
        "gpt3_context = \"\"\"\n",
        "objects = [red block, yellow block, blue block, green bowl]\n",
        "# move all the blocks to the top left corner.\n",
        "robot.pick_and_place(blue block, top left corner)\n",
        "robot.pick_and_place(red block, top left corner)\n",
        "robot.pick_and_place(yellow block, top left corner)\n",
        "done()\n",
        "\n",
        "objects = [red block, yellow block, blue block, green bowl]\n",
        "# put the yellow one the green thing.\n",
        "robot.pick_and_place(yellow block, green bowl)\n",
        "done()\n",
        "\n",
        "objects = [yellow block, blue block, red block]\n",
        "# move the light colored block to the middle.\n",
        "robot.pick_and_place(yellow block, middle)\n",
        "done()\n",
        "\n",
        "objects = [blue block, green bowl, red block, yellow bowl, green block]\n",
        "# stack the blocks.\n",
        "robot.pick_and_place(green block, blue block)\n",
        "robot.pick_and_place(red block, green block)\n",
        "done()\n",
        "\n",
        "objects = [red block, blue block, green bowl, blue bowl, yellow block, green block]\n",
        "# group the blue objects together.\n",
        "robot.pick_and_place(blue block, blue bowl)\n",
        "done()\n",
        "\n",
        "objects = [green bowl, red block, green block, red bowl, yellow bowl, yellow block]\n",
        "# sort all the blocks into their matching color bowls.\n",
        "robot.pick_and_place(green block, green bowl)\n",
        "robot.pick_and_place(red block, red bowl)\n",
        "robot.pick_and_place(yellow block, yellow bowl)\n",
        "done()\n",
        "\"\"\"\n",
        "\n",
        "use_environment_description = False\n",
        "gpt3_context_lines = gpt3_context.split(\"\\n\")\n",
        "gpt3_context_lines_keep = []\n",
        "for gpt3_context_line in gpt3_context_lines:\n",
        "  if \"objects =\" in gpt3_context_line and not use_environment_description:\n",
        "    continue\n",
        "  gpt3_context_lines_keep.append(gpt3_context_line)\n",
        "\n",
        "gpt3_context = \"\\n\".join(gpt3_context_lines_keep)\n",
        "print(gpt3_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCJckkuwFzDS"
      },
      "outputs": [],
      "source": [
        "#@title Task and Config\n",
        "only_plan = False\n",
        "\n",
        "raw_input = \"put all the blocks in different corners.\" \n",
        "config = {\"pick\":  [\"red block\", \"yellow block\", \"green block\", \"blue block\"],\n",
        "          \"place\": [\"red bowl\"]}\n",
        "\n",
        "# raw_input = \"move the block to the bowl.\"\n",
        "# config = {'pick':  ['red block'],\n",
        "#           'place': ['green bowl']}\n",
        "\n",
        "# raw_input = \"put any blocks on their matched colored bowls.\"\n",
        "# config = {'pick':  ['yellow block', 'green block', 'blue block'],\n",
        "#           'place': ['yellow bowl', 'green bowl', 'blue bowl']}\n",
        "          \n",
        "# raw_input = \"put all the blocks in the green bowl.\"\n",
        "# config = {'pick':  ['yellow block', 'green block', 'red block'],\n",
        "#           'place': ['yellow bowl', 'green bowl']}\n",
        "\n",
        "# raw_input = \"stack all the blocks.\"\n",
        "# config = {'pick':  ['yellow block', 'blue block', 'red block'],\n",
        "#           'place': ['blue bowl', 'red bowl']}\n",
        "\n",
        "# raw_input = \"make the highest block stack.\"\n",
        "# config = {'pick':  ['yellow block', 'blue block', 'red block'],\n",
        "#           'place': ['blue bowl', 'red bowl']}\n",
        "\n",
        "# raw_input = \"stack all the blocks.\"\n",
        "# config = {'pick':  ['green block', 'blue block', 'red block'],\n",
        "#           'place': ['yellow bowl', 'green bowl']}\n",
        "\n",
        "# raw_input = \"put the block in all the corners.\" \n",
        "# config = {'pick':  ['red block'],\n",
        "#           'place': ['red bowl', 'green bowl']}\n",
        "\n",
        "# raw_input = \"clockwise, move the block through all the corners.\"\n",
        "# config = {'pick':  ['red block'],\n",
        "#           'place': ['red bowl', 'green bowl', 'yellow bowl']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX18LmbIIGfz"
      },
      "outputs": [],
      "source": [
        "#@title Setup Scene\n",
        "image_path = \"./2db.png\"\n",
        "np.random.seed(2)\n",
        "if config is None:\n",
        "  pick_items = list(PICK_TARGETS.keys())\n",
        "  pick_items = np.random.choice(pick_items, size=np.random.randint(1, 5), replace=False)\n",
        "\n",
        "  place_items = list(PLACE_TARGETS.keys())[:-9]\n",
        "  place_items = np.random.choice(place_items, size=np.random.randint(1, 6 - len(pick_items)), replace=False)\n",
        "  config = {\"pick\":  pick_items,\n",
        "            \"place\": place_items}\n",
        "  print(pick_items, place_items)\n",
        "\n",
        "obs = env.reset(config)\n",
        "\n",
        "img_top = env.get_camera_image_top()\n",
        "img_top_rgb = cv2.cvtColor(img_top, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img_top)\n",
        "\n",
        "imageio.imsave(image_path, img_top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4Yh-PqZ-03n"
      },
      "outputs": [],
      "source": [
        "#@title Runner\n",
        "plot_on = True\n",
        "max_tasks = 5\n",
        "\n",
        "options = make_options(PICK_TARGETS, PLACE_TARGETS, termination_string=termination_string)\n",
        "found_objects = vild(image_path, category_name_string, vild_params, plot_on=False)\n",
        "scene_description = build_scene_description(found_objects)\n",
        "env_description = scene_description\n",
        "\n",
        "print(scene_description)\n",
        "\n",
        "gpt3_prompt = gpt3_context\n",
        "if use_environment_description:\n",
        "  gpt3_prompt += \"\\n\" + env_description\n",
        "gpt3_prompt += \"\\n# \" + raw_input + \"\\n\"\n",
        "\n",
        "all_llm_scores = []\n",
        "all_affordance_scores = []\n",
        "all_combined_scores = []\n",
        "affordance_scores = affordance_scoring(options, found_objects, block_name=\"box\", bowl_name=\"circle\", verbose=False)\n",
        "num_tasks = 0\n",
        "selected_task = \"\"\n",
        "steps_text = []\n",
        "while not selected_task == termination_string:\n",
        "  num_tasks += 1\n",
        "  if num_tasks > max_tasks:\n",
        "    break\n",
        "\n",
        "  llm_scores, _ = gpt3_scoring(gpt3_prompt, options, verbose=True, engine=ENGINE, print_tokens=False)\n",
        "  combined_scores = {option: np.exp(llm_scores[option]) * affordance_scores[option] for option in options}\n",
        "  combined_scores = normalize_scores(combined_scores)\n",
        "  selected_task = max(combined_scores, key=combined_scores.get)\n",
        "  steps_text.append(selected_task)\n",
        "  print(num_tasks, \"Selecting: \", selected_task)\n",
        "  gpt3_prompt += selected_task + \"\\n\"\n",
        "\n",
        "  all_llm_scores.append(llm_scores)\n",
        "  all_affordance_scores.append(affordance_scores)\n",
        "  all_combined_scores.append(combined_scores)\n",
        "\n",
        "if plot_on:\n",
        "  for llm_scores, affordance_scores, combined_scores, step in zip(\n",
        "      all_llm_scores, all_affordance_scores, all_combined_scores, steps_text):\n",
        "    plot_saycan(llm_scores, affordance_scores, combined_scores, step, show_top=10)\n",
        "\n",
        "print('**** Solution ****')\n",
        "print(env_description)\n",
        "print('# ' + raw_input)\n",
        "for i, step in enumerate(steps_text):\n",
        "  if step == '' or step == termination_string:\n",
        "    break\n",
        "  print('Step ' + str(i) + ': ' + step)\n",
        "  nlp_step = step_to_nlp(step)\n",
        "\n",
        "if not only_plan:\n",
        "  print('Initial state:')\n",
        "  plt.imshow(env.get_camera_image())\n",
        "\n",
        "  for i, step in enumerate(steps_text):\n",
        "    if step == '' or step == termination_string:\n",
        "      break\n",
        "    nlp_step = step_to_nlp(step)\n",
        "    print('GPT-3 says next step:', nlp_step)\n",
        "\n",
        "    obs = run_cliport(obs, nlp_step)\n",
        "\n",
        "  # Show camera image after task.\n",
        "  print('Final state:')\n",
        "  plt.imshow(env.get_camera_image())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWQwjuiDi1nC"
      },
      "source": [
        "## Socratic Model: VILD, GPT3, CLIPort\n",
        "\n",
        "This implements a version of LLM planning shown in [Socratic Models](https://socraticmodels.github.io/), without the grounding, but with a scene description. For this relatively simple environment with clear robotic affordances, the scene description is generally sufficient. This mirrors the implementation attached to the paper [here](https://github.com/google-research/google-research/tree/master/socraticmodels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trXP07uypWL8"
      },
      "outputs": [],
      "source": [
        "#@title Prompt\n",
        "\n",
        "gpt3_context = \"\"\"\n",
        "objects = [red block, yellow block, blue block, green bowl]\n",
        "# move all the blocks to the top left corner.\n",
        "robot.pick_and_place(blue block, top left corner)\n",
        "robot.pick_and_place(red block, top left corner)\n",
        "robot.pick_and_place(yellow block, top left corner)\n",
        "done()\n",
        "\n",
        "objects = [red block, yellow block, blue block, green bowl]\n",
        "# put the yellow one the green thing.\n",
        "robot.pick_and_place(yellow block, green bowl)\n",
        "done()\n",
        "\n",
        "objects = [yellow block, blue block, red block]\n",
        "# move the light colored block to the middle.\n",
        "robot.pick_and_place(yellow block, middle)\n",
        "done()\n",
        "\n",
        "objects = [blue block, green bowl, red block, yellow bowl, green block]\n",
        "# stack the blocks.\n",
        "robot.pick_and_place(green block, blue block)\n",
        "robot.pick_and_place(red block, green block)\n",
        "done()\n",
        "\n",
        "objects = [red block, blue block, green bowl, blue bowl, yellow block, green block]\n",
        "# group the blue objects together.\n",
        "robot.pick_and_place(blue block, blue bowl)\n",
        "done()\n",
        "\n",
        "objects = [green bowl, red block, green block, red bowl, yellow bowl, yellow block]\n",
        "# sort all the blocks into their matching color bowls.\n",
        "robot.pick_and_place(green block, green bowl)\n",
        "robot.pick_and_place(red block, red bowl)\n",
        "robot.pick_and_place(yellow block, yellow bowl)\n",
        "done()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPx--Ea7IZ69"
      },
      "outputs": [],
      "source": [
        "#@title Queries and Configs\n",
        "\n",
        "only_plan = False\n",
        "\n",
        "raw_input = \"put all the blocks in different corners.\" \n",
        "config = {'pick':  ['red block', 'yellow block', 'green block', 'blue block'],\n",
        "          'place': ['red bowl']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6TSgRItpPC4"
      },
      "outputs": [],
      "source": [
        "#@title Runner\n",
        "\n",
        "env_description = ''\n",
        "image_path = './2db.png'\n",
        "\n",
        "np.random.seed(2)\n",
        "\n",
        "if config is None:\n",
        "  pick_items = list(PICK_TARGETS.keys())\n",
        "  pick_items = np.random.choice(pick_items, size=np.random.randint(1, 5), replace=False)\n",
        "\n",
        "  place_items = list(PLACE_TARGETS.keys())[:-9]\n",
        "  place_items = np.random.choice(place_items, size=np.random.randint(1, 6 - len(pick_items)), replace=False)\n",
        "  config = {'pick':  pick_items,\n",
        "            'place': place_items}\n",
        "  print(pick_items, place_items)\n",
        "obs = env.reset(config)\n",
        "\n",
        "img_top = env.get_camera_image_top()\n",
        "img_top_rgb = cv2.cvtColor(img_top, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img_top_rgb)\n",
        "\n",
        "imageio.imsave(image_path, img_top)\n",
        "\n",
        "found_objects = vild(image_path, category_name_string, vild_params, plot_on=False)\n",
        "scene_description = build_scene_description(found_objects)\n",
        "print(scene_description)\n",
        "\n",
        "env_description = scene_description\n",
        "\n",
        "gpt3_prompt = gpt3_context\n",
        "gpt3_prompt += \"\\n\" + env_description + \"\\n\"\n",
        "gpt3_prompt += \"# \" + raw_input\n",
        "response = gpt3_call(engine=ENGINE, prompt=gpt3_prompt, max_tokens=128, temperature=0)\n",
        "steps_text = [text.strip().strip() for text in response[\"choices\"][0][\"text\"].strip().split(\"#\")[0].split(\"\\n\")][:-1]\n",
        "print('**** Solution ****')\n",
        "print(env_description)\n",
        "print('# ' + raw_input)\n",
        "for i, step in enumerate(steps_text):\n",
        "  if step == '' or step == termination_string:\n",
        "    break\n",
        "  print('Step ' + str(i) + ': ' + step)\n",
        "  nlp_step = step_to_nlp(step)\n",
        "\n",
        "if not only_plan:\n",
        "  print('Initial state:')\n",
        "  plt.imshow(env.get_camera_image())\n",
        "\n",
        "  for i, step in enumerate(steps_text):\n",
        "    if step == '' or step == termination_string:\n",
        "      break\n",
        "    nlp_step = step_to_nlp(step)\n",
        "    print('GPT-3 says next step:', nlp_step)\n",
        "\n",
        "    obs = run_cliport(obs, nlp_step)\n",
        "\n",
        "  # Show camera image after task.\n",
        "  print('Final state:')\n",
        "  plt.imshow(env.get_camera_image())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SayCan_Robot_Pick_Place.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
